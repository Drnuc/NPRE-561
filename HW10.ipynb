{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior probabilities: [0.1260803521359797, 0.8739196478640202, 7.160908626951125e-37]\n",
      "Weighted average occurrence rate: 0.004382206274533698\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import poisson\n",
    "\n",
    "# Prior probabilities\n",
    "prior_probs = [1/3, 1/3, 1/3]\n",
    "\n",
    "# Occurrence rates given by the experts\n",
    "rates = [1.0e-4, 5.0e-3, 0.05]\n",
    "\n",
    "# Number of earthquakes observed\n",
    "k = 3\n",
    "\n",
    "# Total time period\n",
    "T = 2000\n",
    "\n",
    "# Calculate likelihoods\n",
    "likelihoods = [poisson.pmf(k, rate*T) for rate in rates]\n",
    "\n",
    "# Calculate unnormalized posterior probabilities\n",
    "post_probs_unnorm = [prior*likelihood for prior, likelihood in zip(prior_probs, likelihoods)]\n",
    "\n",
    "# Normalize the posterior probabilities\n",
    "post_probs = [prob/sum(post_probs_unnorm) for prob in post_probs_unnorm]\n",
    "\n",
    "# Compute the weighted average of the occurrence rates\n",
    "weighted_avg_rate = sum(post_prob*rate for post_prob, rate in zip(post_probs, rates))\n",
    "\n",
    "print(f\"Posterior probabilities: {post_probs}\")\n",
    "print(f\"Weighted average occurrence rate: {weighted_avg_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of at least one earthquake: 0.23120611589189344\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import poisson\n",
    "\n",
    "# Weighted average occurrence rate from part a\n",
    "rate = 0.004382206274533698\n",
    "\n",
    "# Total time period\n",
    "T = 60\n",
    "\n",
    "# Calculate the probability of zero earthquakes\n",
    "prob_zero = poisson.pmf(0, rate*T)\n",
    "\n",
    "# Calculate the probability of at least one earthquake\n",
    "prob_at_least_one = 1 - prob_zero\n",
    "\n",
    "print(f\"Probability of at least one earthquake: {prob_at_least_one}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point estimate of λ [1/year]: 0.047619047619047616\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "years_of_operation = [35, 40, 30]\n",
    "number_of_fires = [3, 2, 0]\n",
    "\n",
    "# Total number of fires\n",
    "total_fires = sum(number_of_fires)\n",
    "\n",
    "# Total years of operation\n",
    "total_years = sum(years_of_operation)\n",
    "\n",
    "# Calculate the MLE for the fire ignition frequency\n",
    "lambda_mle = total_fires / total_years\n",
    "\n",
    "print(f\"Point estimate of λ [1/year]: {lambda_mle}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior probabilities for each failure probability: [[0.2580640321113161, 0.6286094003312103, 0.1133265675574736], [0.90830488814569, 0.08989764490936077, 0.0017974669449492796]]\n",
      "Posterior probabilities for each distribution: [0.4978854292393337, 0.5021145707606663]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "# Prior probabilities and failure probabilities per demand\n",
    "prior_probs = [0.5, 0.5]\n",
    "failure_probs = [[0.01, 0.03, 0.05], [0.02, 0.05, 0.10]]\n",
    "prob_masses = [[0.25, 0.50, 0.25], [0.60, 0.20, 0.20]]\n",
    "\n",
    "# Test data\n",
    "n = 100\n",
    "k = 2\n",
    "\n",
    "# Calculate likelihoods\n",
    "likelihoods = [[binom.pmf(k, n, p) for p in failure_probs[i]] for i in range(2)]\n",
    "\n",
    "# Calculate unnormalized posterior probabilities for each failure probability\n",
    "post_probs_unnorm = [[prob_mass * likelihood for prob_mass, likelihood in zip(prob_masses[i], likelihoods[i])] for i in range(2)]\n",
    "\n",
    "# Normalize the posterior probabilities for each failure probability\n",
    "post_probs = [[prob/sum(post_probs_unnorm[i]) for prob in post_probs_unnorm[i]] for i in range(2)]\n",
    "\n",
    "# Calculate the total unnormalized posterior probabilities for each distribution\n",
    "total_post_probs_unnorm = [prior_prob * sum(post_probs_unnorm[i]) for i, prior_prob in enumerate(prior_probs)]\n",
    "\n",
    "# Normalize the posterior probabilities for each distribution\n",
    "total_post_probs = [prob/sum(total_post_probs_unnorm) for prob in total_post_probs_unnorm]\n",
    "\n",
    "print(f\"Posterior probabilities for each failure probability: {post_probs}\")\n",
    "print(f\"Posterior probabilities for each distribution: {total_post_probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu: -4.092744766213293\n",
      "sigma: 0.5780060137126292\n",
      "Acceptance rate: 0.2774\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import uniform, poisson, norm\n",
    "\n",
    "# Field data\n",
    "years_of_operation = np.array([35., 40., 30.])\n",
    "number_of_fires = np.array([3., 2., 0.])\n",
    "\n",
    "# Prior distributions for mu and sigma\n",
    "prior_mu = uniform(loc=-7.0, scale=3.0)  # Uniform between -7 and -4\n",
    "prior_sigma = uniform(loc=0.5, scale=1.0)  # Uniform between 0.5 and 1.5\n",
    "\n",
    "# Log likelihood function\n",
    "def log_likelihood(mu, sigma, years_of_operation, number_of_fires):\n",
    "    lambda_ = np.exp(norm.rvs(loc=mu, scale=sigma, size=len(years_of_operation)))\n",
    "    return poisson.logpmf(number_of_fires, mu=lambda_ * years_of_operation).sum()\n",
    "\n",
    "# Metropolis-Hastings algorithm\n",
    "def metropolis_hastings(prior_mu, prior_sigma, log_likelihood, iterations, initial_values, years_of_operation, number_of_fires):\n",
    "    mu_current, sigma_current = initial_values\n",
    "    accepts = 0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        mu_proposal = norm.rvs(loc=mu_current, scale=0.5)\n",
    "        sigma_proposal = norm.rvs(loc=sigma_current, scale=0.5)\n",
    "\n",
    "        # Check if proposed sigma is within the support of the prior\n",
    "        if sigma_proposal < 0.5 or sigma_proposal > 1.5:\n",
    "            continue\n",
    "\n",
    "        likelihood_current = log_likelihood(mu_current, sigma_current, years_of_operation, number_of_fires)\n",
    "        likelihood_proposal = log_likelihood(mu_proposal, sigma_proposal, years_of_operation, number_of_fires)\n",
    "\n",
    "        prior_current = prior_mu.logpdf(mu_current) + prior_sigma.logpdf(sigma_current)\n",
    "        prior_proposal = prior_mu.logpdf(mu_proposal) + prior_sigma.logpdf(sigma_proposal)\n",
    "\n",
    "        p_current = likelihood_current + prior_current\n",
    "        p_proposal = likelihood_proposal + prior_proposal\n",
    "\n",
    "        p_accept = np.exp(p_proposal - p_current)\n",
    "\n",
    "        accept = p_accept > np.random.rand()\n",
    "        if accept:\n",
    "            mu_current = mu_proposal\n",
    "            sigma_current = sigma_proposal\n",
    "            accepts += 1\n",
    "\n",
    "    return mu_current, sigma_current, accepts / iterations\n",
    "\n",
    "# Run the Metropolis-Hastings algorithm\n",
    "mu, sigma, acceptance_rate = metropolis_hastings(prior_mu, prior_sigma, log_likelihood, 5000, (-5.5, 1.0), years_of_operation, number_of_fires)\n",
    "\n",
    "print(f\"mu: {mu}\")\n",
    "print(f\"sigma: {sigma}\")\n",
    "print(f\"Acceptance rate: {acceptance_rate}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
